# -*- coding: utf-8 -*-
"""FloodPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CdH8QjM_nIbsZu6inuPrEO0MTOq69Tg0
"""

#Import some basic libraries
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

dataset = pd.read_csv("kerala.csv")
#Using data.head() we can see the top 5 rows of the dataset
dataset.head()

dataset.columns



#Now we will cheak if any colomns is left empty
dataset.apply(lambda x:sum(x.isnull()), axis=0)

#replace the yes/no in floods coloumn by 1/0
dataset['FLOODS'].replace(['YES','NO'],[1,0],inplace=True)

dataset.head()

#Now let's seperate the data which we are gonna use for prediction
x = dataset.iloc[:,1:14]
x.head()

#Now seperate the flood label from the dataset
y = dataset.iloc[:, -1]
y.head()

# Commented out IPython magic to ensure Python compatibility.
#Let's see hoe the rainfall index vary during rainy season
import matplotlib.pyplot as plt
# %matplotlib inline
c = dataset[['JUN','JUL','AUG','SEP']]
c.hist()
plt.show()

from sklearn import preprocessing
minmax = preprocessing.MinMaxScaler(feature_range=(0,1))
minmax.fit_transform(x)

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=1)

"""Prediction Algorithms:"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

def mymodel(model):
    model.fit(xtrain, ytrain)
    ypred = model.predict(xtest)

    train = model.score(xtrain, ytrain)
    test = model.score(xtest, ytest)

    print(f"Training Accuracy : {train}\nTesting Accuracy : {test}\n\n")
    print(classification_report(ytest, ypred))

    return model

"""1. KNN Alogrithm

"""

knn = mymodel(KNeighborsClassifier())

"""2. Logistic Regression Alogrithm"""

logreg = mymodel(LogisticRegression())



"""3. Support Vector Machine Alogrithm"""

svm = mymodel(SVC())

"""4. Decision Tree Alogrithm"""

dt = mymodel(DecisionTreeClassifier())

"""Cross-validation"""

from sklearn.model_selection import cross_val_score

knn_accuracy = cross_val_score(knn,xtest,ytest,cv=3,scoring='accuracy',n_jobs=-1)
knn_accuracy.mean()

logreg_accuracy = cross_val_score(logreg,xtest,ytest,cv=3,scoring='accuracy',n_jobs=-1)
logreg_accuracy.mean()

svm_accuracy = cross_val_score(svm,xtest,ytest,cv=3,scoring='accuracy',n_jobs=-1)
svm_accuracy.mean()

dt_accuracy = cross_val_score(dt,xtest,ytest,cv=3,scoring='accuracy',n_jobs=-1)
dt_accuracy.mean()

names = ['KNN','LogReg','SVM','DecisionTree']
score =[knn_accuracy.mean(),logreg_accuracy.mean(),svm_accuracy.mean(),dt_accuracy.mean()]

scores = pd.DataFrame({'Algorithm Name':names,'Score':score})

import seaborn as sns

axis = sns.barplot(x='Algorithm Name',y='Score',data = scores)
axis.set(xlabel='Classifier', ylabel='Accuracy')

for p in axis.patches:
    height = p.get_height()
    axis.text(p.get_x() + p.get_width()/2, height + 0.005, '{:1.4f}'.format(height), ha="center")

scores # Cross-validation